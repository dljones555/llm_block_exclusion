{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtTd1kbvqBFJzMLCToM3sr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "\n",
        "# Triton kernel for initial exclusion logic and block-level masking\n",
        "# This kernel processes header data to determine which tokens are 'compatible'\n",
        "# and computes a block-level mask for optimization.\n",
        "@triton.jit\n",
        "def exclude_kernel(\n",
        "    headers_ptr,       # Pointer to the input header data (e.g., POS tags, affordances, energy, age)\n",
        "    mask_ptr,          # Output pointer for the token-level exclusion mask (1=keep, 0=exclude)\n",
        "    block_mast_ptr,    # Output pointer for the block-level exclusion mask (1=keep block, 0=exclude block)\n",
        "    seq_len,           # Total sequence length\n",
        "    BLOCK_SIZE: tl.constexpr, # Size of the processing block for each program instance\n",
        "):\n",
        "    # Get the program ID (block ID) for the current Triton program instance\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    # Calculate global offsets for tokens within this block\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    # Create a mask to ensure we don't access out-of-bounds tokens at the end of the sequence\n",
        "    mask = offsets < seq_len\n",
        "\n",
        "    # Load headers for the current block. We assume 4 rows of header data packed sequentially.\n",
        "    # headers_ptr points to the start of 'h_type', then 'h_afford' starts at seq_len offset, etc.\n",
        "    h_type   = tl.load(headers_ptr + offsets, mask=mask, other=0)\n",
        "    h_afford = tl.load(headers_ptr + seq_len + offsets, mask=mask, other=0)\n",
        "    h_energy = tl.load(headers_ptr + 2 * seq_len + offsets, mask=mask, other=0)\n",
        "    h_age    = tl.load(headers_ptr + 3 * seq_len + offsets, mask=mask, other=0)\n",
        "\n",
        "    # Apply a simple exclusion logic based on header values.\n",
        "    # For demonstration: compatible if (type AND affordance are non-zero) AND (energy >= 3) AND (age <= 5)\n",
        "    compat = ((h_type & h_afford) != 0) & (h_energy >= 3) & (h_age <= 5)\n",
        "\n",
        "    # Store the token-level compatibility mask (1 if compatible, 0 if excluded).\n",
        "    tl.store(mask_ptr + offsets, compat.to(tl.int32), mask=mask)\n",
        "\n",
        "    # Compute the block-level mask: if any token in the block is compatible, the block should be kept.\n",
        "    # This allows for early exiting of entire blocks in subsequent kernels.\n",
        "    block_keep = tl.sum(compat.to(tl.int32), axis=0) > 0\n",
        "\n",
        "    # Store the block-level mask. Each block has one block_keep value.\n",
        "    tl.store(block_mast_ptr + pid, block_keep.to(tl.int32))\n",
        "\n",
        "# Triton kernel for attention computation with integrated exclusion logic and block pruning.\n",
        "# This kernel performs scaled dot-product attention but only for compatible tokens and blocks.\n",
        "@triton.jit\n",
        "def exclude_attention_kernel(\n",
        "    q_ptr,             # Pointer to the Query tensor\n",
        "    k_ptr,             # Pointer to the Key tensor\n",
        "    v_ptr,             # Pointer to the Value tensor\n",
        "    headers_ptr,       # Pointer to the header data (same as exclude_kernel)\n",
        "    mask_ptr,          # Output pointer for the token-level exclusion mask (recomputed or loaded)\n",
        "    block_mask_ptr,    # Input pointer for the block-level exclusion mask (computed by exclude_kernel)\n",
        "    output_ptr,        # Output pointer for the final attention result\n",
        "    seq_len,           # Total sequence length\n",
        "    d_head: tl.constexpr,   # Dimension of each attention head\n",
        "    BLOCK_SIZE: tl.constexpr # Size of the processing block\n",
        "):\n",
        "    pid = tl.program_id(0)  # Program ID (block index)\n",
        "\n",
        "    # Compute offsets for the current block along the sequence length\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask_seq = offsets < seq_len  # Mask out-of-bounds offsets for sequence length\n",
        "\n",
        "    # Load headers for the current block to re-evaluate or confirm token compatibility\n",
        "    h_type   = tl.load(headers_ptr + offsets, mask=mask_seq, other=0)\n",
        "    h_afford = tl.load(headers_ptr + seq_len + offsets, mask=mask_seq, other=0)\n",
        "    h_energy = tl.load(headers_ptr + 2 * seq_len + offsets, mask=mask_seq, other=0)\n",
        "    h_age    = tl.load(headers_ptr + 3 * seq_len + offsets, mask=mask_seq, other=0)\n",
        "\n",
        "    # Simple exclusion logic (vectorized)\n",
        "    compat = ((h_type & h_afford) != 0) & (h_energy >= 3) & (h_age <= 5)\n",
        "\n",
        "    # Store the exclusion mask (token level) for this block\n",
        "    tl.store(mask_ptr + offsets, compat.to(tl.int32), mask=mask_seq)\n",
        "\n",
        "    # Load the block-level mask, which was pre-computed by exclude_kernel.\n",
        "    # This allows skipping entire blocks if they contain no compatible tokens.\n",
        "    block_keep = tl.load(block_mask_ptr + pid)\n",
        "\n",
        "    # If the block is entirely excluded (no compatible tokens), skip all attention computation for it.\n",
        "    if block_keep == 0:\n",
        "        return\n",
        "\n",
        "    # Load Q, K, V tensors for the current block.\n",
        "    # The batch and head dimensions are flattened, so we load directly from the sequence dimension.\n",
        "    q_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    q = tl.load(q_ptr + q_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    k_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    k = tl.load(k_ptr + k_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    v_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    v = tl.load(v_ptr + v_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    # Compute attention scores (scaled dot-product attention)\n",
        "    scores = tl.dot(q, tl.trans(k)) / (d_head**0.5)\n",
        "\n",
        "    # Apply exclusion mask to scores: incompatible tokens should not contribute to attention.\n",
        "    # Expand compat to match score dimensions for element-wise multiplication.\n",
        "    compat_expanded = compat[:, None].to(tl.float32)\n",
        "    scores = scores * compat_expanded\n",
        "\n",
        "    # Softmax implementation (equivalent to F.softmax(scores, dim=-1) in Triton)\n",
        "    # Subtract max for numerical stability\n",
        "    scores_max = tl.max(scores, axis=1)[:, None]\n",
        "    exp_scores = tl.math.exp(scores - scores_max)\n",
        "\n",
        "    # Apply mask again after exponentiation to ensure excluded tokens remain zeroed out\n",
        "    exp_scores = exp_scores * compat_expanded\n",
        "\n",
        "    # Sum exponentials and divide to get attention weights\n",
        "    sum_exp_scores = tl.sum(exp_scores, axis=1)[:, None]\n",
        "    # Handle potential division by zero if all values in a row were masked out\n",
        "    attn_weights = tl.where(sum_exp_scores > 0, exp_scores / sum_exp_scores, 0.0)\n",
        "\n",
        "    # Apply attention weights to the Value tensor\n",
        "    result = tl.dot(attn_weights, v)\n",
        "\n",
        "    # Store the final attention result for the current block\n",
        "    result_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    tl.store(output_ptr + result_offsets, result, mask=mask_seq[:, None])\n",
        "\n",
        "\n",
        "# Python function to test baseline (standard PyTorch) attention performance\n",
        "def test_baseline(seq_len=64, device=\"cuda\"):\n",
        "\n",
        "    print(f\"Testing baseline with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    d_head = 64\n",
        "\n",
        "    # Initialize dummy Query and Key tensors\n",
        "    q = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    k = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    # Value tensor is also needed for scaled_dot_product_attention, even if not explicitly passed to matmul\n",
        "    v = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # Use PyTorch's native scaled_dot_product_attention for a fair comparison\n",
        "    # This function handles the matmul, scaling, softmax, and multiplication with V\n",
        "    dummy_attn = torch.nn.functional.scaled_dot_product_attention(\n",
        "        q, k, v, is_causal=False\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Baseline full matmul time: {end - start:.6f}s\")\n",
        "\n",
        "# Python function to test the custom Triton attention kernel with exclusion\n",
        "def test_kernel_attention(seq_len=64, device=\"cuda\"):\n",
        "    print(f\"Test_kernel_attention with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    d_head = 64\n",
        "    BLOCK_SIZE = 32 # Define the block size for Triton kernels\n",
        "\n",
        "    # Initialize input tensors (Q, K, V) with random data\n",
        "    q = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    k = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    v = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "\n",
        "    # Headers: Simulate part-of-speech related metadata or other exclusion criteria.\n",
        "    # Stored as a flat tensor, 4 categories (type, afford, energy, age), each of seq_len.\n",
        "    # To ensure more blocks are skipped, we will make the conditions for 'compat' less likely to be met.\n",
        "\n",
        "    # headers = torch.randint(0, 10, (4 * seq_len,), device=device, dtype=torch.int32)\n",
        "    # Generate header components separately\n",
        "    h_type_data = torch.randint(0, 10, (seq_len,), device=device, dtype=torch.int32)\n",
        "    h_afford_data = torch.randint(0, 10, (seq_len,), device=device, dtype=torch.int32)\n",
        "    # Make h_energy have a mix of values >=3 and <3\n",
        "    h_energy_data = torch.randint(0, 5, (seq_len,), device=device, dtype=torch.int32) # Range [0, 4]\n",
        "    # Make h_age have a mix of values <=5 and >5\n",
        "    h_age_data = torch.randint(0, 8, (seq_len,), device=device, dtype=torch.int32)    # Range [0, 7]\n",
        "\n",
        "    # Concatenate them to form the headers tensor as expected by the kernel\n",
        "    headers = torch.cat([h_type_data, h_afford_data, h_energy_data, h_age_data])\n",
        "\n",
        "    # Token-level mask: will store 1 or 0 for each token indicating compatibility.\n",
        "    mask = torch.zeros(seq_len, device=device, dtype=torch.int32)\n",
        "\n",
        "    # Output tensor for storing the final attention result from the Triton kernel.\n",
        "    output = torch.zeros_like(q)\n",
        "\n",
        "    # Block-level mask: Stores 1 or 0 for each block (seq_len / BLOCK_SIZE blocks).\n",
        "    # This mask determines if an entire block can be skipped.\n",
        "    num_blocks = triton.cdiv(seq_len, BLOCK_SIZE)\n",
        "    block_mask = torch.zeros(num_blocks, device=device, dtype=torch.int32)\n",
        "\n",
        "    # Define the grid for Triton kernel launch (one program per block)\n",
        "    grid = (num_blocks,)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # Step 1: Run the `exclude_kernel` to populate the token-level and block-level masks.\n",
        "    # This kernel identifies compatible tokens and sets the 'block_mask' for optimization.\n",
        "    exclude_kernel[grid](\n",
        "        headers,\n",
        "        mask,\n",
        "        block_mask, # Populated by exclude_kernel\n",
        "        seq_len,\n",
        "        BLOCK_SIZE=BLOCK_SIZE\n",
        "    )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    print(f\"Exclusion kernel time: {end - start:.6f}s\")\n",
        "    print(f\"Block mask (first 100 values): {block_mask[:100].tolist()}...\")\n",
        "    print(f\"Number of blocks to process (after exclusion): {block_mask.sum().item()}\")\n",
        "    print(f\"Total blocks: {len(block_mask)}\")\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # Step 2: Run the `exclude_attention_kernel`.\n",
        "    # This kernel performs attention, leveraging the 'block_mask' for early exits\n",
        "    # and applying the token-level exclusion logic during score calculation and softmax.\n",
        "    exclude_attention_kernel[grid](\n",
        "        q, k, v, headers, mask, block_mask, output, seq_len, d_head, BLOCK_SIZE=BLOCK_SIZE\n",
        "    )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    print(f\"Kernel time (exclusion + attention): {end - start:.6f}s\")\n",
        "\n",
        "\n",
        "# Loop through different sequence lengths to test performance scaling\n",
        "for seq_len in [64, 2048, 4096, 32768]:\n",
        "  test_baseline(seq_len)\n",
        "  test_kernel_attention(seq_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FegDTYNu8jc",
        "outputId": "142834a1-54a7-42ea-e170-541c83deefc3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing baseline with seq_len=64\n",
            "-----\n",
            "Baseline full matmul time: 0.000220s\n",
            "Test_kernel_attention with seq_len=64\n",
            "-----\n",
            "Exclusion kernel time: 0.005018s\n",
            "Block mask (first 100 values): [1, 1]...\n",
            "Number of blocks to process (after exclusion): 2\n",
            "Total blocks: 2\n",
            "Kernel time (exclusion + attention): 0.004723s\n",
            "Testing baseline with seq_len=2048\n",
            "-----\n",
            "Baseline full matmul time: 0.001172s\n",
            "Test_kernel_attention with seq_len=2048\n",
            "-----\n",
            "Exclusion kernel time: 0.000068s\n",
            "Block mask (first 100 values): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
            "Number of blocks to process (after exclusion): 64\n",
            "Total blocks: 64\n",
            "Kernel time (exclusion + attention): 0.000102s\n",
            "Testing baseline with seq_len=4096\n",
            "-----\n",
            "Baseline full matmul time: 0.003628s\n",
            "Test_kernel_attention with seq_len=4096\n",
            "-----\n",
            "Exclusion kernel time: 0.000058s\n",
            "Block mask (first 100 values): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
            "Number of blocks to process (after exclusion): 126\n",
            "Total blocks: 128\n",
            "Kernel time (exclusion + attention): 0.000135s\n",
            "Testing baseline with seq_len=32768\n",
            "-----\n",
            "Baseline full matmul time: 0.186147s\n",
            "Test_kernel_attention with seq_len=32768\n",
            "-----\n",
            "Exclusion kernel time: 0.000104s\n",
            "Block mask (first 100 values): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
            "Number of blocks to process (after exclusion): 1019\n",
            "Total blocks: 1024\n",
            "Kernel time (exclusion + attention): 0.000640s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CqDcsaS31HMj"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}