{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSCv7DFJnYwntJq4Xycgct",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dljones555/llm_block_exclusion/blob/main/block_level_exclusion_mve.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "\n",
        "@triton.jit\n",
        "def exclude_kernel(\n",
        "    headers_ptr,\n",
        "    mask_ptr,\n",
        "    block_mast_ptr,\n",
        "    seq_len,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < seq_len\n",
        "\n",
        "    # Load headers (4 rows packed sequentially)\n",
        "    h_type   = tl.load(headers_ptr + offsets, mask=mask, other=0)\n",
        "    h_afford = tl.load(headers_ptr + seq_len + offsets, mask=mask, other=0)\n",
        "    h_energy = tl.load(headers_ptr + 2 * seq_len + offsets, mask=mask, other=0)\n",
        "    h_age    = tl.load(headers_ptr + 3 * seq_len + offsets, mask=mask, other=0)\n",
        "\n",
        "    # Simple exclusion logic (vectorized)\n",
        "    compat = ((h_type & h_afford) != 0) & (h_energy >= 3) & (h_age <= 5)\n",
        "\n",
        "    # Store 1 = keep, 0 = excluded\n",
        "    tl.store(mask_ptr + offsets, compat.to(tl.int32), mask=mask)\n",
        "\n",
        "@triton.jit\n",
        "def exclude_attention_kernel(\n",
        "    q_ptr, k_ptr, v_ptr, headers_ptr, mask_ptr, block_mask_ptr, output_ptr,\n",
        "    seq_len, d_head: tl.constexpr, BLOCK_SIZE: tl.constexpr\n",
        "):\n",
        "    pid = tl.program_id(0)  # Program ID (block)\n",
        "\n",
        "    # Compute offsets for the current block along the sequence length\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask_seq = offsets < seq_len  # Mask out-of-bounds offsets for sequence length\n",
        "\n",
        "    # Load headers (4 rows packed sequentially)\n",
        "    h_type   = tl.load(headers_ptr + offsets, mask=mask_seq, other=0)\n",
        "    h_afford = tl.load(headers_ptr + seq_len + offsets, mask=mask_seq, other=0)\n",
        "    h_energy = tl.load(headers_ptr + 2 * seq_len + offsets, mask=mask_seq, other=0)\n",
        "    h_age    = tl.load(headers_ptr + 3 * seq_len + offsets, mask=mask_seq, other=0)\n",
        "\n",
        "    # Simple exclusion logic (vectorized)\n",
        "    compat = ((h_type & h_afford) != 0) & (h_energy >= 3) & (h_age <= 5)\n",
        "\n",
        "    # Store the exclusion mask (token level)\n",
        "    tl.store(mask_ptr + offsets, compat.to(tl.int32), mask=mask_seq)\n",
        "\n",
        "    # Load the block-level mask (whether the block is kept or not)\n",
        "    block_keep = tl.load(block_mask_ptr + pid)\n",
        "\n",
        "    # If the block is excluded, skip the entire block (early exit)\n",
        "    if block_keep == 0:\n",
        "        return\n",
        "\n",
        "    # Load Q, K, V tensors (flattened batch and head dimensions)\n",
        "    q_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    q = tl.load(q_ptr + q_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    k_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    k = tl.load(k_ptr + k_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    v_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    v = tl.load(v_ptr + v_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    # Compute attention scores (scaled dot-product attention)\n",
        "    scores = tl.dot(q, tl.trans(k)) / (d_head**0.5)\n",
        "\n",
        "    # Apply exclusion mask (0 for excluded, no contribution to scores from these positions)\n",
        "    compat_expanded = compat[:, None].to(tl.float32)\n",
        "    scores = scores * compat_expanded\n",
        "\n",
        "    # Softmax (implementing F.softmax(scores, dim=-1) in Triton)\n",
        "    scores_max = tl.max(scores, axis=1)[:, None]\n",
        "    exp_scores = tl.math.exp(scores - scores_max)\n",
        "\n",
        "    # Apply mask again after exponentiation to ensure excluded tokens are zero\n",
        "    exp_scores = exp_scores * compat_expanded\n",
        "\n",
        "    sum_exp_scores = tl.sum(exp_scores, axis=1)[:, None]\n",
        "    attn_weights = exp_scores / sum_exp_scores\n",
        "\n",
        "    # Apply attention to V\n",
        "    result = tl.dot(attn_weights, v)\n",
        "\n",
        "    # Store final attention result\n",
        "    result_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    tl.store(output_ptr + result_offsets, result, mask=mask_seq[:, None])\n",
        "\n",
        "\n",
        "def test_baseline(seq_len=64, device=\"cuda\"):\n",
        "\n",
        "    print(f\"Testing baseline with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    d_head = 64\n",
        "\n",
        "    q = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    k = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # pure matmul timing - no softmax\n",
        "    dummy_scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "    # dummy_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_head)\n",
        "\n",
        "    # dummy_attn = F.softmax(dummy_scores, dim=-1)\n",
        "    dummy_attn = torch.nn.functional.scaled_dot_product_attention(\n",
        "        q, k, k, is_causal=False\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Baseline full matmul time: {end - start:.6f}s\")\n",
        "\n",
        "def test_kernel_attention(seq_len=64, device=\"cuda\"):\n",
        "    print(f\"Test_kernel_attention with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    d_head = 64\n",
        "\n",
        "    # Initialize input tensors (Q, K, V)\n",
        "    q = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    k = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    v = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "\n",
        "    # Headers (to simulate your POS-related metadata)\n",
        "    headers = torch.randint(0, 10, (4 * seq_len,), device=device, dtype=torch.int32)\n",
        "\n",
        "    # Mask for storing the exclusion mask result\n",
        "    mask = torch.zeros(seq_len, device=device, dtype=torch.int32)\n",
        "\n",
        "    # Output tensor for storing the final attention result\n",
        "    output = torch.zeros_like(q)\n",
        "\n",
        "    # Block-level mask for excluding blocks\n",
        "    block_mask = torch.zeros(seq_len // 32, device=device, dtype=torch.int32)  # Assuming BLOCK_SIZE=32\n",
        "\n",
        "    BLOCK_SIZE = 32\n",
        "    grid = (triton.cdiv(seq_len, BLOCK_SIZE),)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # First, run the exclusion kernel to generate the block mask\n",
        "    exclude_kernel[grid](\n",
        "        headers,\n",
        "        mask,\n",
        "        block_mask,\n",
        "        seq_len,\n",
        "        BLOCK_SIZE=BLOCK_SIZE\n",
        "    )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    print(f\"Exclusion kernel time: {end - start:.6f}s\")\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # Now, run the exclusion + attention kernel\n",
        "    exclude_attention_kernel[grid](\n",
        "        q, k, v, headers, mask, block_mask, output, seq_len, d_head, BLOCK_SIZE=BLOCK_SIZE\n",
        "    )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    print(f\"Kernel time (exclusion + attention): {end - start:.6f}s\")\n",
        "\n",
        "\n",
        "for seq_len in [64, 2048, 4096, 32768]:\n",
        "  test_baseline(seq_len)\n",
        "  test_kernel_attention(seq_len)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FegDTYNu8jc",
        "outputId": "34b121cc-b1e5-43b3-bc01-d7a0e41a7b17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing baseline with seq_len=64\n",
            "-----\n",
            "Baseline full matmul time: 0.000445s\n",
            "Test_kernel_attention with seq_len=64\n",
            "-----\n",
            "Exclusion kernel time: 1.895744s\n",
            "Kernel time (exclusion + attention): 0.860382s\n",
            "Testing baseline with seq_len=2048\n",
            "-----\n",
            "Baseline full matmul time: 0.005819s\n",
            "Test_kernel_attention with seq_len=2048\n",
            "-----\n",
            "Exclusion kernel time: 0.000161s\n",
            "Kernel time (exclusion + attention): 0.000103s\n",
            "Testing baseline with seq_len=4096\n",
            "-----\n",
            "Baseline full matmul time: 0.005012s\n",
            "Test_kernel_attention with seq_len=4096\n",
            "-----\n",
            "Exclusion kernel time: 0.000145s\n",
            "Kernel time (exclusion + attention): 0.000108s\n",
            "Testing baseline with seq_len=32768\n",
            "-----\n",
            "Baseline full matmul time: 0.246926s\n",
            "Test_kernel_attention with seq_len=32768\n",
            "-----\n",
            "Exclusion kernel time: 0.000100s\n",
            "Kernel time (exclusion + attention): 0.000074s\n"
          ]
        }
      ]
    }
  ]
}