{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "collapsed_sections": [
        "ffab2a52"
      ],
      "authorship_tag": "ABX9TyNYZUirl1F9aSVEr5vx9Gz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dljones555/llm_block_exclusion/blob/main/test_block_level_sparsity_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgE-Tr751uXN"
      },
      "outputs": [],
      "source": [
        "# c:\\projects\\select-attention\\tests\\test_phase1_profile.py\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoConfig\n",
        "\n",
        "def test_can_extract_attention():\n",
        "    \"\"\"Extract ATTENTION WEIGHTS (not outputs).\"\"\"\n",
        "\n",
        "    # Load config and ensure output_attentions is True\n",
        "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "    config.output_attentions = True\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "    print(f\"Model config output_attentions: {model.config.output_attentions}\")\n",
        "\n",
        "    # Forward pass\n",
        "    input_ids = tokenizer.encode(\n",
        "        \"The quick brown fox jumps over the lazy dog\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    print(f\"Input shape: {input_ids.shape}\")\n",
        "    print(f\"Input text: {tokenizer.decode(input_ids[0])}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Pass output_attentions=True to the model to get attention weights in the output\n",
        "        outputs = model(input_ids, output_attentions=True)\n",
        "\n",
        "    print(f\"Type of outputs: {type(outputs)}\")\n",
        "    print(f\"Has 'attentions' attribute: {hasattr(outputs, 'attentions')}\")\n",
        "    if hasattr(outputs, 'attentions'):\n",
        "        print(f\"Length of outputs.attentions: {len(outputs.attentions) if outputs.attentions is not None else 'None'}\")\n",
        "        if outputs.attentions:\n",
        "            for i, att in enumerate(outputs.attentions):\n",
        "                print(f\"  Layer {i} attention type: {type(att)}\")\n",
        "                print(f\"  Layer {i} attention shape: {att.shape if att is not None else 'None'}\")\n",
        "\n",
        "    # Attention weights are in outputs.attentions (a tuple of tensors, one per layer)\n",
        "    if not hasattr(outputs, 'attentions') or not outputs.attentions:\n",
        "        print(\"❌ No attention weights found in model output. Check output_attentions=True.\")\n",
        "        return False\n",
        "\n",
        "    attn = None\n",
        "    for i, layer_attn in enumerate(outputs.attentions):\n",
        "        if layer_attn is not None: # Ensure the tensor itself is not None\n",
        "            attn = layer_attn.detach().cpu()\n",
        "            print(f\"✓ Extracted attention weights from layer {i}.\")\n",
        "            break\n",
        "\n",
        "    if attn is None:\n",
        "        print(\"❌ All attention layers returned None or were empty. Cannot extract attention weights.\")\n",
        "        return False\n",
        "\n",
        "    print(f\"\\n✓ Attention matrix shape: {attn.shape}\")\n",
        "    print(f\"✓ Attention range: [{attn.min():.4f}, {attn.max():.4f}]\")\n",
        "    print(f\"✓ Attention sum per row (should be ~1.0): {attn[0, 0].sum(dim=-1)}\")\n",
        "\n",
        "    # Analyze block sparsity\n",
        "    seq_len = attn.shape[-1]\n",
        "\n",
        "    # Per-position: what fraction of attending positions have weight < 0.05?\n",
        "    sparse_mask = (attn < 0.05).float()\n",
        "    sparsity_per_pos = sparse_mask.mean(dim=-1)  # [batch, heads, seq]\n",
        "\n",
        "    print(f\"\\n--- SPARSITY ANALYSIS ---\")\n",
        "    print(f\"Sparsity per position (mean): {sparsity_per_pos.mean():.2%}\")\n",
        "    print(f\"Sparsity per position (min): {sparsity_per_pos.min():.2%}\")\n",
        "    print(f\"Sparsity per position (max): {sparsity_per_pos.max():.2%}\")\n",
        "\n",
        "    # Block-level: group into 4-token blocks\n",
        "    block_size = 4\n",
        "    num_blocks = (seq_len + block_size - 1) // block_size\n",
        "\n",
        "    block_max = []\n",
        "    for i in range(0, seq_len, block_size):\n",
        "        end = min(i + block_size, seq_len)\n",
        "        block_attn = attn[..., i:end]  # [batch, heads, seq, block_size]\n",
        "        block_max.append(block_attn.max(dim=-1).values)\n",
        "\n",
        "    block_max = torch.stack(block_max, dim=-1)  # [batch, heads, seq, num_blocks]\n",
        "\n",
        "    # What fraction of blocks are negligible (max weight < 0.1)?\n",
        "    negligible_mask = (block_max < 0.1).float()\n",
        "    negligible_per_head = negligible_mask.mean(dim=-2)  # [batch, heads, num_blocks]\n",
        "\n",
        "    print(f\"\\n--- BLOCK-LEVEL SPARSITY (block_size={block_size}) ---\")\n",
        "    print(f\"Negligible blocks (<0.1): {negligible_per_head.mean():.2%}\")\n",
        "    print(f\"Potentially prunable: {negligible_per_head.mean() > 0.10}\")\n",
        "\n",
        "    for threshold in [0.05, 0.10, 0.20, 0.50]:\n",
        "        prunable = (block_max < threshold).float().mean()\n",
        "        print(f\"  Blocks < {threshold:.2f}: {prunable:.2%}\")\n",
        "\n",
        "    # Save\n",
        "    torch.save(attn, \"phase1_attention_weights.pt\")\n",
        "    print(f\"\\n✓ Saved to phase1_attention_weights.pt\")\n",
        "\n",
        "    return True\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if test_can_extract_attention():\n",
        "        print(\"\\n✅ PHASE 1 PASSED: Can extract real attention weights\")\n",
        "    else:\n",
        "        print(\"\\n❌ PHASE 1 FAILED: Debug above\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c:\\projects\\select-attention\\tests\\test_phase2_quality.py\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoConfig\n",
        "\n",
        "def test_quality_with_pruning():\n",
        "    \"\"\"\n",
        "    Run same prompt twice:\n",
        "    1. Full attention (baseline)\n",
        "    2. Pruned attention (blocks < 0.1 set to 0)\n",
        "\n",
        "    Compare outputs: are they the same?\n",
        "    \"\"\"\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "    config.output_attentions = True\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt = \"The quick brown fox jumps over the lazy dog\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Step 1: Baseline (full attention)\n",
        "    print(\"Step 1: BASELINE INFERENCE (full attention)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_baseline = model(input_ids, output_attentions=True)\n",
        "\n",
        "    baseline_logits = outputs_baseline.logits  # [1, 9, 50257]\n",
        "    baseline_tokens = baseline_logits.argmax(dim=-1)\n",
        "\n",
        "    print(f\"Baseline logits shape: {baseline_logits.shape}\")\n",
        "    print(f\"Baseline predicted tokens: {baseline_tokens[0].tolist()}\")\n",
        "    print(f\"Baseline decoded: {tokenizer.decode(baseline_tokens[0])}\")\n",
        "\n",
        "    # Step 2: Extract attention weights\n",
        "    print(\"\\nStep 2: EXTRACT ATTENTION WEIGHTS\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    attentions = outputs_baseline.attentions  # List of [1, 12, 9, 9]\n",
        "\n",
        "    # Prune: set blocks with max weight < 0.1 to 0\n",
        "    pruning_threshold = 0.1\n",
        "    pruned_attentions = []\n",
        "\n",
        "    total_blocks = 0\n",
        "    pruned_blocks = 0\n",
        "\n",
        "    for layer_idx, attn in enumerate(attentions):\n",
        "        # attn: [batch, heads, seq, seq]\n",
        "        attn_pruned = attn.clone()\n",
        "\n",
        "        # Find blocks (4-token groups) with negligible attention\n",
        "        seq_len = attn.shape[-1]\n",
        "        block_size = 4\n",
        "\n",
        "        for i in range(0, seq_len, block_size):\n",
        "            end = min(i + block_size, seq_len)\n",
        "            block_attn = attn_pruned[..., i:end]\n",
        "            block_max = block_attn.max(dim=-1, keepdim=True).values\n",
        "\n",
        "            # Prune: where block_max < threshold, set to 0\n",
        "            mask = (block_max < pruning_threshold).float()\n",
        "            block_attn_masked = block_attn * (1 - mask)\n",
        "\n",
        "            # Renormalize attention (so rows still sum to 1)\n",
        "            row_sum = block_attn_masked.sum(dim=-1, keepdim=True)\n",
        "            row_sum = row_sum.clamp(min=1e-8)\n",
        "            block_attn_renorm = block_attn_masked / row_sum\n",
        "\n",
        "            attn_pruned[..., i:end] = block_attn_renorm\n",
        "\n",
        "            total_blocks += attn.shape[1] * attn.shape[2] * ((end - i) // block_size or 1)\n",
        "            pruned_blocks += (mask.sum().item())\n",
        "\n",
        "        pruned_attentions.append(attn_pruned)\n",
        "\n",
        "    print(f\"Pruned {pruned_blocks}/{total_blocks} blocks ({pruned_blocks/total_blocks:.1%})\")\n",
        "\n",
        "    # Step 3: Inject pruned attentions into model (simulate)\n",
        "    print(\"\\nStep 3: QUALITY IMPACT ANALYSIS\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Note: We can't directly inject pruned attentions into the forward pass.\n",
        "    # Instead, measure attention divergence:\n",
        "\n",
        "    attention_divergence = []\n",
        "    for layer_idx, (attn_orig, attn_pruned) in enumerate(zip(attentions, pruned_attentions)):\n",
        "        # KL divergence between original and pruned\n",
        "        kl = torch.nn.functional.kl_div(\n",
        "            torch.log(attn_pruned + 1e-10),\n",
        "            attn_orig,\n",
        "            reduction='batchmean'\n",
        "        )\n",
        "        attention_divergence.append(kl.item())\n",
        "        print(f\"Layer {layer_idx}: KL divergence = {kl.item():.4f}\")\n",
        "\n",
        "    mean_kl = sum(attention_divergence) / len(attention_divergence)\n",
        "    print(f\"\\nMean KL divergence across layers: {mean_kl:.4f}\")\n",
        "\n",
        "    if mean_kl < 0.1:\n",
        "        print(\"✅ Pruning preserves attention distribution (KL < 0.1)\")\n",
        "    else:\n",
        "        print(\"⚠️ Pruning changes attention significantly (KL >= 0.1)\")\n",
        "\n",
        "    # Step 4: Estimate output divergence\n",
        "    print(\"\\nStep 4: ESTIMATED OUTPUT IMPACT\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Rough estimate: attention divergence correlates with logit divergence\n",
        "    # If attention KL is low, logits should be similar\n",
        "\n",
        "    estimated_logit_error = mean_kl * 100  # Very rough heuristic\n",
        "    print(f\"Estimated logit error: ~{estimated_logit_error:.1f}%\")\n",
        "\n",
        "    if mean_kl < 0.05:\n",
        "        print(\"✅ SAFE TO PRUNE: Minimal quality loss expected\")\n",
        "    elif mean_kl < 0.10:\n",
        "        print(\"⚠️ MODERATE RISK: Some quality loss may occur\")\n",
        "    else:\n",
        "        print(\"❌ HIGH RISK: Significant quality loss likely\")\n",
        "\n",
        "    return {\n",
        "        \"baseline_logits\": baseline_logits,\n",
        "        \"baseline_tokens\": baseline_tokens,\n",
        "        \"attention_divergence\": attention_divergence,\n",
        "        \"mean_kl\": mean_kl,\n",
        "        \"pruned_blocks_pct\": pruned_blocks / total_blocks,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = test_quality_with_pruning()\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"PHASE 2 SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Blocks pruned: {results['pruned_blocks_pct']:.1%}\")\n",
        "    print(f\"Attention divergence: {results['mean_kl']:.4f}\")\n",
        "    print(f\"Verdict: {'✅ SAFE' if results['mean_kl'] < 0.05 else '⚠️ RISKY'}\")"
      ],
      "metadata": {
        "id": "5DAZPxxN724R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a3849f9"
      },
      "source": [
        "# Task\n",
        "Modify the `test_quality_with_pruning` function in `test_phase2_quality.py` to iterate over various `pruning_threshold` values (0.01, 0.005, 0.001, 0.0005). For each threshold, execute the pruning and quality analysis, reporting the mean KL divergence and the percentage of pruned blocks. The goal is to identify a threshold that results in a mean KL divergence below 0.1, indicating minimal quality loss while achieving sparsity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "042033ba"
      },
      "source": [
        "## Review Current Pruning Impact\n",
        "\n",
        "### Subtask:\n",
        "Analyze the current pruning approach in `test_phase2_quality.py`. Specifically, explain why a `pruning_threshold` of 0.1, leading to over 50% of blocks being pruned and subsequently renormalized, results in such a high mean KL divergence (38.33) and indicates significant quality loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8cdee55"
      },
      "source": [
        "### Analysis of Pruning Impact and High KL Divergence\n",
        "\n",
        "The `test_quality_with_pruning` function in `test_phase2_quality.py` demonstrates a block-level pruning strategy with a `pruning_threshold` of 0.1. The executed output shows that **58.1% of blocks were pruned**, leading to a **mean KL divergence of 38.3316** across the attention layers. This significantly high KL divergence indicates a substantial alteration of the attention distribution, which is flagged as a \"HIGH RISK\" for quality loss.\n",
        "\n",
        "Here's a breakdown of why this occurs:\n",
        "\n",
        "1.  **Aggressive Pruning Threshold (0.1):** A `pruning_threshold` of 0.1 means that any 4-token attention block where the *maximum* attention weight within that block is less than 0.1 is considered \"negligible\" and its contribution is effectively zeroed out. Given that even the `test_can_extract_attention` function (Phase 1) showed significant sparsity (e.g., 60.39% mean sparsity per position and 53.70% of blocks < 0.1), setting such a threshold will naturally lead to a large proportion of blocks being masked.\n",
        "\n",
        "2.  **Zeroing out Blocks (`block_attn * (1 - mask)`):** When `mask` is 1 (meaning the block is negligible), the term `(1 - mask)` becomes 0, and `block_attn_masked` for that block becomes all zeros. This directly removes a substantial amount of information from the original attention distribution. For 58.1% of the blocks to be pruned, it implies that over half of the attention connections, even those with small but potentially significant weights, are completely discarded.\n",
        "\n",
        "3.  **Renormalization (`block_attn_renorm = block_attn_masked / row_sum`):** After zeroing out these blocks, the remaining non-zero attention weights in each row are *renormalized*. This step forces the pruned attention weights for each query token to still sum to 1. However, since a large portion of the original weights have been removed, the renormalization process amplifies the remaining non-zero weights. For example, if a row originally had 10 attention values, and 6 of them (belonging to pruned blocks) were set to zero, the remaining 4 values must now collectively sum to 1. This means their individual values will be scaled up significantly compared to their original values.\n",
        "\n",
        "4.  **Consequence: Divergence from Original Distribution:**\n",
        "    *   The combination of aggressive zeroing and subsequent amplification of remaining weights fundamentally changes the *relative importance* of different tokens. What was a small weight in the original distribution might become a larger, more prominent weight in the pruned and renormalized distribution simply because other weights were removed.\n",
        "    *   **KL Divergence** measures how one probability distribution diverges from a second, expected probability distribution. A high KL divergence (like 38.33) precisely quantifies this drastic change. It indicates that the `attn_pruned` distribution is very different from the `attn_orig` distribution, not just in magnitude but in its overall shape and the relative probabilities assigned to different attended tokens.\n",
        "\n",
        "In essence, by removing over half of the attention information and then scaling up the rest, the pruned attention mechanism is likely focusing on entirely different or fewer key tokens compared to the original model. This massive shift in attention patterns inevitably leads to different internal representations and, consequently, different output logits, resulting in a high estimated quality loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f89a9666"
      },
      "source": [
        "## Iterate on Pruning Thresholds\n",
        "\n",
        "### Subtask:\n",
        "Modify the `test_quality_with_pruning` function in `test_phase2_quality.py` to allow iterating over a set of `pruning_threshold` values (e.g., 0.01, 0.005, 0.001, 0.0005). For each threshold, execute the pruning and quality analysis, reporting the mean KL divergence and the percentage of pruned blocks. The goal is to find a threshold that keeps the mean KL divergence below 0.1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bb9186b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `test_quality_with_pruning` function and its main execution block to iterate through different pruning thresholds. I will provide a `code_block` that includes the complete updated Python file with the necessary changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f92db93"
      },
      "source": [
        "# c:\\projects\\select-attention\\tests\\test_phase2_quality.py\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoConfig\n",
        "\n",
        "def test_quality_with_pruning(pruning_threshold):\n",
        "    \"\"\"\n",
        "    Run same prompt twice:\n",
        "    1. Full attention (baseline)\n",
        "    2. Pruned attention (blocks < pruning_threshold set to 0)\n",
        "\n",
        "    Compare outputs: are they the same?\n",
        "    \"\"\"\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "    config.output_attentions = True\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt = \"The quick brown fox jumps over the lazy dog\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Step 1: Baseline (full attention)\n",
        "    # print(\"Step 1: BASELINE INFERENCE (full attention)\")\n",
        "    # print(\"-\" * 50)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_baseline = model(input_ids, output_attentions=True)\n",
        "\n",
        "    baseline_logits = outputs_baseline.logits  # [1, 9, 50257]\n",
        "    baseline_tokens = baseline_logits.argmax(dim=-1)\n",
        "\n",
        "    # print(f\"Baseline logits shape: {baseline_logits.shape}\")\n",
        "    # print(f\"Baseline predicted tokens: {baseline_tokens[0].tolist()}\")\n",
        "    # print(f\"Baseline decoded: {tokenizer.decode(baseline_tokens[0])}\")\n",
        "\n",
        "    # Step 2: Extract attention weights\n",
        "    # print(\"\\nStep 2: EXTRACT ATTENTION WEIGHTS\")\n",
        "    # print(\"-\" * 50)\n",
        "\n",
        "    attentions = outputs_baseline.attentions  # List of [1, 12, 9, 9]\n",
        "\n",
        "    # Prune: set blocks with max weight < pruning_threshold to 0\n",
        "    pruned_attentions = []\n",
        "\n",
        "    total_blocks = 0\n",
        "    pruned_blocks = 0\n",
        "\n",
        "    for layer_idx, attn in enumerate(attentions):\n",
        "        # attn: [batch, heads, seq, seq]\n",
        "        attn_pruned = attn.clone()\n",
        "\n",
        "        # Find blocks (4-token groups) with negligible attention\n",
        "        seq_len = attn.shape[-1]\n",
        "        block_size = 4\n",
        "\n",
        "        for i in range(0, seq_len, block_size):\n",
        "            end = min(i + block_size, seq_len)\n",
        "            # Ensure block_attn has at least one element along the last dim\n",
        "            if i < end:\n",
        "                block_attn = attn_pruned[..., :, i:end]\n",
        "                block_max = block_attn.max(dim=-1, keepdim=True).values\n",
        "\n",
        "                # Prune: where block_max < threshold, set to 0\n",
        "                mask = (block_max < pruning_threshold).float()\n",
        "                block_attn_masked = block_attn * (1 - mask)\n",
        "\n",
        "                # Renormalize attention (so rows still sum to 1)\n",
        "                row_sum = block_attn_masked.sum(dim=-1, keepdim=True)\n",
        "                row_sum = row_sum.clamp(min=1e-8)\n",
        "                block_attn_renorm = block_attn_masked / row_sum\n",
        "\n",
        "                attn_pruned[..., :, i:end] = block_attn_renorm\n",
        "\n",
        "                total_blocks += attn.shape[1] * attn.shape[2]\n",
        "                pruned_blocks += mask.sum().item()\n",
        "\n",
        "        pruned_attentions.append(attn_pruned)\n",
        "\n",
        "    # print(f\"Pruned {pruned_blocks}/{total_blocks} blocks ({pruned_blocks/total_blocks:.1%})\")\n",
        "\n",
        "    # Step 3: Inject pruned attentions into model (simulate)\n",
        "    # print(\"\\nStep 3: QUALITY IMPACT ANALYSIS\")\n",
        "    # print(\"-\" * 50)\n",
        "\n",
        "    # Note: We can't directly inject pruned attentions into the forward pass.\n",
        "    # Instead, measure attention divergence:\n",
        "\n",
        "    attention_divergence = []\n",
        "    for layer_idx, (attn_orig, attn_pruned) in enumerate(zip(attentions, pruned_attentions)):\n",
        "        # KL divergence between original and pruned\n",
        "        # Add a small epsilon to attn_pruned before log to avoid log(0)\n",
        "        kl = torch.nn.functional.kl_div(\n",
        "            torch.log(attn_pruned + 1e-10),\n",
        "            attn_orig,\n",
        "            reduction='batchmean'\n",
        "        )\n",
        "        attention_divergence.append(kl.item())\n",
        "        # print(f\"Layer {layer_idx}: KL divergence = {kl.item():.4f}\")\n",
        "\n",
        "    mean_kl = sum(attention_divergence) / len(attention_divergence)\n",
        "    # print(f\"\\nMean KL divergence across layers: {mean_kl:.4f}\")\n",
        "\n",
        "    # if mean_kl < 0.1:\n",
        "    #     print(\"✅ Pruning preserves attention distribution (KL < 0.1)\")\n",
        "    # else:\n",
        "    #     print(\"⚠️ Pruning changes attention significantly (KL >= 0.1)\")\n",
        "\n",
        "    # Step 4: Estimate output divergence\n",
        "    # print(\"\\nStep 4: ESTIMATED OUTPUT IMPACT\")\n",
        "    # print(\"-\" * 50)\n",
        "\n",
        "    # Rough estimate: attention divergence correlates with logit divergence\n",
        "    # If attention KL is low, logits should be similar\n",
        "\n",
        "    # estimated_logit_error = mean_kl * 100  # Very rough heuristic\n",
        "    # print(f\"Estimated logit error: ~{estimated_logit_error:.1f}%\")\n",
        "\n",
        "    # if mean_kl < 0.05:\n",
        "    #     print(\"✅ SAFE TO PRUNE: Minimal quality loss expected\")\n",
        "    # elif mean_kl < 0.10:\n",
        "    #     print(\"⚠️ MODERATE RISK: Some quality loss may occur\")\n",
        "    # else:\n",
        "    #     print(\"❌ HIGH RISK: Significant quality loss likely\")\n",
        "\n",
        "    return {\n",
        "        \"baseline_logits\": baseline_logits,\n",
        "        \"baseline_tokens\": baseline_tokens,\n",
        "        \"attention_divergence\": attention_divergence,\n",
        "        \"mean_kl\": mean_kl,\n",
        "        \"pruned_blocks_pct\": pruned_blocks / total_blocks if total_blocks > 0 else 0,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a list of pruning_threshold values to test\n",
        "    pruning_thresholds_to_test = [0.01, 0.005, 0.001, 0.0005]\n",
        "\n",
        "    for threshold in pruning_thresholds_to_test:\n",
        "        print(\"=\"*50)\n",
        "        print(f\"EVALUATING PRUNING THRESHOLD: {threshold}\")\n",
        "        print(\"=\"*50)\n",
        "        results = test_quality_with_pruning(threshold)\n",
        "        print(f\"Blocks pruned: {results['pruned_blocks_pct']:.1%}\")\n",
        "        print(f\"Attention divergence (mean KL): {results['mean_kl']:.4f}\")\n",
        "        if results['mean_kl'] < 0.1:\n",
        "            print(\"Verdict: ✅ Acceptable KL divergence (below 0.1)\")\n",
        "        else:\n",
        "            print(\"Verdict: ❌ High KL divergence (above 0.1)\")\n",
        "        print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cbb70d9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution output showed negative KL divergence values, which is mathematically incorrect as KL divergence must always be non-negative. This indicates an issue with the interpretation or numerical stability of `torch.nn.functional.kl_div` in this context. I will replace the `kl_div` function call with a manual calculation of KL divergence to ensure non-negative results and correct interpretation (D_KL(P || Q)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f78318e7"
      },
      "source": [
        "# c:\\projects\\select-attention\\tests\\test_phase2_quality.py\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoConfig\n",
        "\n",
        "def test_quality_with_pruning(pruning_threshold):\n",
        "    \"\"\"\n",
        "    Run same prompt twice:\n",
        "    1. Full attention (baseline)\n",
        "    2. Pruned attention (blocks < pruning_threshold set to 0)\n",
        "\n",
        "    Compare outputs: are they the same?\n",
        "    \"\"\"\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "    config.output_attentions = True\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt = \"The quick brown fox jumps over the lazy dog\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_baseline = model(input_ids, output_attentions=True)\n",
        "\n",
        "    baseline_logits = outputs_baseline.logits  # [1, 9, 50257]\n",
        "    baseline_tokens = baseline_logits.argmax(dim=-1)\n",
        "\n",
        "    attentions = outputs_baseline.attentions  # List of [1, 12, 9, 9]\n",
        "\n",
        "    pruned_attentions = []\n",
        "\n",
        "    total_blocks = 0\n",
        "    pruned_blocks = 0\n",
        "\n",
        "    for layer_idx, attn in enumerate(attentions):\n",
        "        # attn: [batch, heads, seq, seq]\n",
        "        attn_pruned = attn.clone()\n",
        "\n",
        "        # Find blocks (4-token groups) with negligible attention\n",
        "        seq_len = attn.shape[-1]\n",
        "        block_size = 4\n",
        "\n",
        "        for i in range(0, seq_len, block_size):\n",
        "            end = min(i + block_size, seq_len)\n",
        "            # Ensure block_attn has at least one element along the last dim\n",
        "            if i < end:\n",
        "                block_attn = attn_pruned[..., :, i:end]\n",
        "                block_max = block_attn.max(dim=-1, keepdim=True).values\n",
        "\n",
        "                # Prune: where block_max < threshold, set to 0\n",
        "                mask = (block_max < pruning_threshold).float()\n",
        "                block_attn_masked = block_attn * (1 - mask)\n",
        "\n",
        "                # Renormalize attention (so rows still sum to 1)\n",
        "                row_sum = block_attn_masked.sum(dim=-1, keepdim=True)\n",
        "                row_sum = row_sum.clamp(min=1e-8)\n",
        "                block_attn_renorm = block_attn_masked / row_sum\n",
        "\n",
        "                attn_pruned[..., :, i:end] = block_attn_renorm\n",
        "\n",
        "                # Count blocks for statistics\n",
        "                total_blocks += attn.shape[1] * attn.shape[2] # Count one 'block set' per query position\n",
        "                pruned_blocks += mask.sum().item() # Count how many query positions had this block pruned\n",
        "\n",
        "        pruned_attentions.append(attn_pruned)\n",
        "\n",
        "    attention_divergence = []\n",
        "    for layer_idx, (attn_orig, attn_pruned) in enumerate(zip(attentions, pruned_attentions)):\n",
        "        # Calculate KL divergence D_KL(P || Q) = sum(P * log(P/Q))\n",
        "        # P = attn_orig (original attention distribution),\n",
        "        # Q = attn_pruned (pruned and renormalized attention distribution)\n",
        "        # Add a small epsilon to both distributions to prevent log(0) and potential NaN/inf results.\n",
        "\n",
        "        epsilon = 1e-10\n",
        "\n",
        "        # Calculate per-element terms for KL divergence\n",
        "        # P * (log(P) - log(Q)) = P * log(P/Q)\n",
        "        kl_divergence_per_element = attn_orig * (torch.log(attn_orig + epsilon) - torch.log(attn_pruned.clamp(min=epsilon)))\n",
        "\n",
        "        # Sum over the last dimension (the 'key' sequence length) to get KL divergence for each query token\n",
        "        kl_per_query = kl_divergence_per_element.sum(dim=-1)\n",
        "\n",
        "        # Average over batch, heads, and query positions to get a single scalar KL for the layer\n",
        "        kl = kl_per_query.mean().item() # Convert to Python scalar\n",
        "        attention_divergence.append(kl)\n",
        "\n",
        "    mean_kl = sum(attention_divergence) / len(attention_divergence)\n",
        "\n",
        "    return {\n",
        "        \"baseline_logits\": baseline_logits,\n",
        "        \"baseline_tokens\": baseline_tokens,\n",
        "        \"attention_divergence\": attention_divergence,\n",
        "        \"mean_kl\": mean_kl,\n",
        "        \"pruned_blocks_pct\": pruned_blocks / total_blocks if total_blocks > 0 else 0,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a list of pruning_threshold values to test\n",
        "    pruning_thresholds_to_test = [0.01, 0.005, 0.001, 0.0005]\n",
        "\n",
        "    for threshold in pruning_thresholds_to_test:\n",
        "        print(\"=\"*50)\n",
        "        print(f\"EVALUATING PRUNING THRESHOLD: {threshold}\")\n",
        "        print(\"=\"*50)\n",
        "        results = test_quality_with_pruning(threshold)\n",
        "        print(f\"Blocks pruned: {results['pruned_blocks_pct']:.1%}\")\n",
        "        print(f\"Attention divergence (mean KL): {results['mean_kl']:.4f}\")\n",
        "        if results['mean_kl'] < 0.1:\n",
        "            print(\"Verdict: ✅ Acceptable KL divergence (below 0.1)\")\n",
        "        else:\n",
        "            print(\"Verdict: ❌ High KL divergence (above 0.1)\")\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8d8dbea"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution output still showed negative KL divergence values, which is mathematically incorrect as KL divergence must always be non-negative. This indicates a numerical instability issue with the calculation, possibly due to extremely small probabilities. I will use a manually implemented KL divergence formula with robust clamping and explicit handling of potential negative results to ensure mathematical correctness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "671cdde7"
      },
      "source": [
        "# c:\\projects\\select-attention\\tests\\test_phase2_quality.py\n",
        "import torch\n",
        "import sys\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoConfig\n",
        "\n",
        "def test_quality_with_pruning(pruning_threshold):\n",
        "    \"\"\"\n",
        "    Run same prompt twice:\n",
        "    1. Full attention (baseline)\n",
        "    2. Pruned attention (blocks < pruning_threshold set to 0)\n",
        "\n",
        "    Compare outputs: are they the same?\n",
        "    \"\"\"\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "    config.output_attentions = True\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt = \"The quick brown fox jumps over the lazy dog\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_baseline = model(input_ids, output_attentions=True)\n",
        "\n",
        "    baseline_logits = outputs_baseline.logits  # [1, 9, 50257]\n",
        "    baseline_tokens = baseline_logits.argmax(dim=-1)\n",
        "\n",
        "    attentions = outputs_baseline.attentions  # List of [1, 12, 9, 9]\n",
        "\n",
        "    pruned_attentions = []\n",
        "\n",
        "    total_blocks = 0\n",
        "    pruned_blocks = 0\n",
        "\n",
        "    for layer_idx, attn in enumerate(attentions):\n",
        "        # attn: [batch, heads, seq, seq]\n",
        "        attn_pruned = attn.clone()\n",
        "\n",
        "        # Find blocks (4-token groups) with negligible attention\n",
        "        seq_len = attn.shape[-1]\n",
        "        block_size = 4\n",
        "\n",
        "        for i in range(0, seq_len, block_size):\n",
        "            end = min(i + block_size, seq_len)\n",
        "            # Ensure block_attn has at least one element along the last dim\n",
        "            if i < end:\n",
        "                block_attn = attn_pruned[..., :, i:end]\n",
        "                block_max = block_attn.max(dim=-1, keepdim=True).values\n",
        "\n",
        "                # Prune: where block_max < threshold, set to 0\n",
        "                mask = (block_max < pruning_threshold).float()\n",
        "                block_attn_masked = block_attn * (1 - mask)\n",
        "\n",
        "                # Renormalize attention (so rows still sum to 1)\n",
        "                row_sum = block_attn_masked.sum(dim=-1, keepdim=True)\n",
        "                row_sum = row_sum.clamp(min=1e-8)\n",
        "                block_attn_renorm = block_attn_masked / row_sum\n",
        "\n",
        "                attn_pruned[..., :, i:end] = block_attn_renorm\n",
        "\n",
        "                # Count blocks for statistics\n",
        "                total_blocks += attn.shape[1] * attn.shape[2] # Count one 'block set' per query position\n",
        "                pruned_blocks += mask.sum().item() # Count how many query positions had this block pruned\n",
        "\n",
        "        pruned_attentions.append(attn_pruned)\n",
        "\n",
        "    attention_divergence = []\n",
        "    for layer_idx, (attn_orig, attn_pruned) in enumerate(zip(attentions, pruned_attentions)):\n",
        "        # Calculate KL divergence D_KL(P || Q) = sum(P * log(P/Q))\n",
        "        # P = attn_orig (original attention distribution),\n",
        "        # Q = attn_pruned (pruned and renormalized attention distribution)\n",
        "        # Clamp both distributions to prevent log(0) and ensure numerical stability.\n",
        "\n",
        "        epsilon = 1e-12 # A very small number\n",
        "\n",
        "        p_dist = attn_orig.clamp(min=epsilon)\n",
        "        q_dist = attn_pruned.clamp(min=epsilon)\n",
        "\n",
        "        # Compute element-wise KL divergence term: P_i * (log(P_i) - log(Q_i))\n",
        "        kl_divergence_per_element = p_dist * (torch.log(p_dist) - torch.log(q_dist))\n",
        "\n",
        "        # Sum over the last dimension (the 'key' sequence length) to get KL divergence for each query token\n",
        "        kl_per_query = kl_divergence_per_element.sum(dim=-1)\n",
        "\n",
        "        # Average over batch, heads, and query positions to get a single scalar KL for the layer\n",
        "        kl = kl_per_query.mean().item()\n",
        "\n",
        "        # KL divergence must be non-negative. Clamp any numerical artifacts to 0.\n",
        "        if kl < 0:\n",
        "            # print(f\"WARNING: Negative KL divergence encountered for layer {layer_idx}: {kl:.4f}. Clamping to 0.\")\n",
        "            kl = 0.0\n",
        "\n",
        "        attention_divergence.append(kl)\n",
        "\n",
        "    mean_kl = sum(attention_divergence) / len(attention_divergence)\n",
        "\n",
        "    return {\n",
        "        \"baseline_logits\": baseline_logits,\n",
        "        \"baseline_tokens\": baseline_tokens,\n",
        "        \"attention_divergence\": attention_divergence,\n",
        "        \"mean_kl\": mean_kl,\n",
        "        \"pruned_blocks_pct\": pruned_blocks / total_blocks if total_blocks > 0 else 0,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a list of pruning_threshold values to test\n",
        "    pruning_thresholds_to_test = [0.01, 0.005, 0.001, 0.0005]\n",
        "\n",
        "    for threshold in pruning_thresholds_to_test:\n",
        "        print(\"=\"*50)\n",
        "        print(f\"EVALUATING PRUNING THRESHOLD: {threshold}\")\n",
        "        print(\"=\"*50)\n",
        "        results = test_quality_with_pruning(threshold)\n",
        "        print(f\"Blocks pruned: {results['pruned_blocks_pct']:.1%}\")\n",
        "        print(f\"Attention divergence (mean KL): {results['mean_kl']:.4f}\")\n",
        "        if results['mean_kl'] < 0.1:\n",
        "            print(\"Verdict: \\u2705 Acceptable KL divergence (below 0.1)\")\n",
        "        else:\n",
        "            print(\"Verdict: \\u274c High KL divergence (above 0.1)\")\n",
        "        print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf040701"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution output showed all KL divergence values as 0.0000, which is mathematically incorrect for pruning that modifies the attention distribution. This suggests numerical instability in the KL divergence calculation. I will refine the manual KL divergence implementation using `torch.where` to explicitly handle cases where original attention weights are zero, ensuring mathematically correct and non-negative values. Additionally, I will correct the `total_blocks` counting to accurately reflect the number of potential blocks across all query positions, heads, and key blocks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ec55b7"
      },
      "source": [
        "# c:\\projects\\select-attention\\tests\\test_phase2_quality.py\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoConfig\n",
        "\n",
        "def test_quality_with_pruning(pruning_threshold):\n",
        "    \"\"\"\n",
        "    Run same prompt twice:\n",
        "    1. Full attention (baseline)\n",
        "    2. Pruned attention (blocks < pruning_threshold set to 0)\n",
        "\n",
        "    Compare outputs: are they the same?\n",
        "    \"\"\"\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "    config.output_attentions = True\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt = \"The quick brown fox jumps over the lazy dog\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_baseline = model(input_ids, output_attentions=True)\n",
        "\n",
        "    baseline_logits = outputs_baseline.logits  # [1, 9, 50257]\n",
        "    baseline_tokens = baseline_logits.argmax(dim=-1)\n",
        "\n",
        "    attentions = outputs_baseline.attentions  # List of [1, 12, 9, 9]\n",
        "\n",
        "    pruned_attentions = []\n",
        "\n",
        "    total_blocks = 0\n",
        "    pruned_blocks = 0\n",
        "\n",
        "    for layer_idx, attn in enumerate(attentions):\n",
        "        # attn: [batch, heads, seq, seq]\n",
        "        attn_pruned = attn.clone()\n",
        "\n",
        "        # Find blocks (4-token groups) with negligible attention\n",
        "        seq_len = attn.shape[-1]\n",
        "        block_size = 4\n",
        "        num_key_blocks_in_layer = (seq_len + block_size - 1) // block_size\n",
        "\n",
        "        for i in range(0, seq_len, block_size):\n",
        "            end = min(i + block_size, seq_len)\n",
        "            if i < end:\n",
        "                block_attn = attn_pruned[..., :, i:end]\n",
        "                block_max = block_attn.max(dim=-1, keepdim=True).values\n",
        "\n",
        "                # Prune: where block_max < threshold, set to 0\n",
        "                mask = (block_max < pruning_threshold).float()\n",
        "                block_attn_masked = block_attn * (1 - mask)\n",
        "\n",
        "                # Renormalize attention (so rows still sum to 1)\n",
        "                row_sum = block_attn_masked.sum(dim=-1, keepdim=True)\n",
        "                row_sum = row_sum.clamp(min=1e-8)\n",
        "                block_attn_renorm = block_attn_masked / row_sum\n",
        "\n",
        "                attn_pruned[..., :, i:end] = block_attn_renorm\n",
        "\n",
        "                # Count blocks for statistics\n",
        "                # A block 'mask' is applied per batch, per head, per query position for each key block segment\n",
        "                pruned_blocks += mask.sum().item()\n",
        "\n",
        "        # Total possible block positions for this layer\n",
        "        total_blocks += attn.shape[0] * attn.shape[1] * attn.shape[2] * num_key_blocks_in_layer\n",
        "\n",
        "        pruned_attentions.append(attn_pruned)\n",
        "\n",
        "    attention_divergence = []\n",
        "    for layer_idx, (attn_orig, attn_pruned) in enumerate(zip(attentions, pruned_attentions)):\n",
        "        # Calculate KL divergence D_KL(P || Q) = sum(P * log(P/Q))\n",
        "        # P = attn_orig (original attention distribution),\n",
        "        # Q = attn_pruned (pruned and renormalized attention distribution)\n",
        "\n",
        "        epsilon = 1e-12\n",
        "\n",
        "        # Ensure Q is never zero for log(Q) to prevent -inf. Clamp Q.\n",
        "        q_dist_clamped = attn_pruned.clamp(min=epsilon)\n",
        "\n",
        "        # Calculate P * log(P/Q). Handle P=0 explicitly as P*log(P/Q) = 0 if P=0.\n",
        "        kl_per_element = torch.where(attn_orig > 0,\n",
        "                                     attn_orig * (torch.log(attn_orig) - torch.log(q_dist_clamped)),\n",
        "                                     torch.zeros_like(attn_orig))\n",
        "\n",
        "        # Sum over the last dimension (the 'key' sequence length) to get KL divergence for each query token\n",
        "        kl_per_query = kl_per_element.sum(dim=-1)\n",
        "\n",
        "        # Average over batch, heads, and query positions to get a single scalar KL for the layer\n",
        "        kl = kl_per_query.mean().item()\n",
        "\n",
        "        attention_divergence.append(kl)\n",
        "\n",
        "    mean_kl = sum(attention_divergence) / len(attention_divergence)\n",
        "\n",
        "    return {\n",
        "        \"baseline_logits\": baseline_logits,\n",
        "        \"baseline_tokens\": baseline_tokens,\n",
        "        \"attention_divergence\": attention_divergence,\n",
        "        \"mean_kl\": mean_kl,\n",
        "        \"pruned_blocks_pct\": pruned_blocks / total_blocks if total_blocks > 0 else 0,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define a list of pruning_threshold values to test\n",
        "    pruning_thresholds_to_test = [0.01, 0.005, 0.001, 0.0005]\n",
        "\n",
        "    for threshold in pruning_thresholds_to_test:\n",
        "        print(\"=\"*50)\n",
        "        print(f\"EVALUATING PRUNING THRESHOLD: {threshold}\")\n",
        "        print(\"=\"*50)\n",
        "        results = test_quality_with_pruning(threshold)\n",
        "        print(f\"Blocks pruned: {results['pruned_blocks_pct']:.1%}\")\n",
        "        print(f\"Attention divergence (mean KL): {results['mean_kl']:.4f}\")\n",
        "        if results['mean_kl'] < 0.1:\n",
        "            print(\"Verdict: \\u2705 Acceptable KL divergence (below 0.1)\")\n",
        "        else:\n",
        "            print(\"Verdict: \\u274c High KL divergence (above 0.1)\")\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74b8d977"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution output still showed negative KL divergence values, which is mathematically incorrect for pruning that modifies the attention distribution. This suggests numerical instability in the KL divergence calculation. The primary issue was an incorrect renormalization process where attention weights were renormalized locally within blocks rather than across the entire attention row, leading to non-probabilistic distributions. I will modify the code to ensure global renormalization of attention weights after pruning and explicitly clamp individual KL divergence values to be non-negative to handle any residual numerical artifacts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78e13fa"
      },
      "source": [
        "# c:\\projects\\select-attention\\tests\\test_phase2_quality.py\n",
        "import torch\n",
        "import sys\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoConfig\n",
        "\n",
        "def test_quality_with_pruning(pruning_threshold, block_size=32):\n",
        "    \"\"\"\n",
        "    Run same prompt twice:\n",
        "    1. Full attention (baseline)\n",
        "    2. Pruned attention (blocks < pruning_threshold set to 0)\n",
        "\n",
        "    Compare outputs: are they the same?\n",
        "    \"\"\"\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\"gpt2\")\n",
        "    config.output_attentions = True\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt = \"\"\"In a quiet village nestled between two mountains, there lived\n",
        "    an old blacksmith named Thomas. For forty years, he had forged swords\n",
        "    and horseshoes in his small shop on the main street. One cold morning,\n",
        "    a young apprentice arrived at his door, trembling from the winter wind,\n",
        "    asking to learn the ancient craft. Thomas, who had never taken an\n",
        "    apprentice before, was deeply hesitant. But something about the boy's\n",
        "    fierce determination reminded him of his own younger self, decades ago.\n",
        "    Over the next several months, the dedicated apprentice learned patience,\n",
        "    precision, and the secrets of metalworking from the master. He practiced\n",
        "    dawn to dusk, his hands growing calloused and strong. When the apprentice\n",
        "    finally forged his first perfect blade, both the old teacher and young\n",
        "    student wept with joy. Thomas realized then that his forty years of\n",
        "    solitude were finally fulfilled through this one worthy successor. The\n",
        "    blacksmith's legacy would continue.\"\"\"\n",
        "    # prompt = \"The quick brown fox jumps over the lazy dog\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs_baseline = model(input_ids, output_attentions=True)\n",
        "\n",
        "    baseline_logits = outputs_baseline.logits  # [1, 9, 50257]\n",
        "    baseline_tokens = baseline_logits.argmax(dim=-1)\n",
        "\n",
        "    attentions = outputs_baseline.attentions  # List of [1, 12, 9, 9]\n",
        "\n",
        "    pruned_attentions = []\n",
        "\n",
        "    pruned_blocks_count = 0\n",
        "\n",
        "    epsilon = 1e-12 # A very small number for numerical stability\n",
        "\n",
        "    for layer_idx, attn_orig_layer in enumerate(attentions):\n",
        "        # attn_orig_layer: [batch, heads, seq_len_q, seq_len_k]\n",
        "        attn_pruned_layer_temp = attn_orig_layer.clone() # Temporary to apply pruning masks\n",
        "\n",
        "        batch_size, num_heads, seq_len_q, seq_len_k = attn_orig_layer.shape\n",
        "        num_key_blocks = (seq_len_k + block_size - 1) // block_size\n",
        "\n",
        "        # Iterate over each query position and each key block segment\n",
        "        for q_idx in range(seq_len_q):\n",
        "            for i in range(0, seq_len_k, block_size):\n",
        "                end = min(i + block_size, seq_len_k)\n",
        "                if i < end:\n",
        "                    # Extract the attention block for this query, for all batches and heads\n",
        "                    # Shape: [batch, heads, block_size]\n",
        "                    current_block_segment = attn_pruned_layer_temp[..., q_idx, i:end]\n",
        "\n",
        "                    # Find the maximum attention weight within this block segment for all heads/batches\n",
        "                    # Shape: [batch, heads, 1]\n",
        "                    block_max = current_block_segment.max(dim=-1, keepdim=True).values\n",
        "\n",
        "                    # Create a mask: 1 where block_max < threshold, 0 otherwise\n",
        "                    # Shape: [batch, heads, 1]\n",
        "                    mask = (block_max < pruning_threshold).float()\n",
        "\n",
        "                    # Apply pruning: if mask is 1, set the corresponding block in the temporary layer to 0\n",
        "                    # Mask needs to be broadcast across the block_size dimension\n",
        "                    attn_pruned_layer_temp[..., q_idx, i:end] = current_block_segment * (1 - mask)\n",
        "\n",
        "                    # Count pruned blocks: mask.sum() sums up the 1s (pruned blocks) across batch and heads\n",
        "                    pruned_blocks_count += mask.sum().item()\n",
        "\n",
        "        # After all blocks in the layer are potentially zeroed out, perform GLOBAL renormalization for each row\n",
        "        # Sum over the key sequence length (last dimension)\n",
        "        row_sums = attn_pruned_layer_temp.sum(dim=-1, keepdim=True) # Shape: [batch, heads, seq_len_q, 1]\n",
        "        row_sums = row_sums.clamp(min=epsilon) # Avoid division by zero\n",
        "\n",
        "        # Divide each element in the row by its row sum to make it a probability distribution\n",
        "        attn_pruned_layer = attn_pruned_layer_temp / row_sums\n",
        "\n",
        "        pruned_attentions.append(attn_pruned_layer)\n",
        "\n",
        "    attention_divergence = []\n",
        "    for layer_idx, (attn_orig, attn_pruned) in enumerate(zip(attentions, pruned_attentions)):\n",
        "        # Calculate KL divergence D_KL(P || Q) = sum(P * log(P/Q))\n",
        "        # P = attn_orig (original attention distribution),\n",
        "        # Q = attn_pruned (pruned and globally renormalized attention distribution)\n",
        "\n",
        "        # Ensure P and Q are never zero for log. Clamp both.\n",
        "        p_dist = attn_orig.clamp(min=epsilon)\n",
        "        q_dist = attn_pruned.clamp(min=epsilon)\n",
        "\n",
        "        # Compute element-wise KL divergence term: P_i * (log(P_i) - log(Q_i))\n",
        "        # Use torch.where to handle cases where attn_orig is effectively zero, making P*log(P/Q) = 0.\n",
        "        kl_per_element = torch.where(p_dist > 0, # Check p_dist after clamping\n",
        "                                     p_dist * (torch.log(p_dist) - torch.log(q_dist)),\n",
        "                                     torch.zeros_like(p_dist))\n",
        "\n",
        "        # Sum over the last dimension (the 'key' sequence length) to get KL divergence for each query token\n",
        "        kl_per_query = kl_per_element.sum(dim=-1) # Shape: [batch, heads, seq_len_q]\n",
        "\n",
        "        # Clamp any negative values to 0.0 due to potential numerical instability before averaging\n",
        "        kl_per_query = kl_per_query.clamp(min=0.0)\n",
        "\n",
        "        # Average over batch, heads, and query positions to get a single scalar KL for the layer\n",
        "        kl = kl_per_query.mean().item()\n",
        "\n",
        "        attention_divergence.append(kl)\n",
        "        # print(f\"Layer {layer_idx}: KL = {kl:.4f}\")\n",
        "\n",
        "    mean_kl = sum(attention_divergence) / len(attention_divergence)\n",
        "\n",
        "    # After computing attention_divergence, add:\n",
        "\n",
        "    print(\"\\n--- PER-POSITION ANALYSIS ---\")\n",
        "    for layer_idx, (attn_orig, attn_pruned) in enumerate(zip(attentions, pruned_attentions)):\n",
        "        p_dist = attn_orig.clamp(min=epsilon)\n",
        "        q_dist = attn_pruned.clamp(min=epsilon)\n",
        "\n",
        "        kl_per_element = torch.where(p_dist > 0,\n",
        "                                    p_dist * (torch.log(p_dist) - torch.log(q_dist)),\n",
        "                                   torch.zeros_like(p_dist))\n",
        "        kl_per_position = kl_per_element.sum(dim=-1).mean(dim=(0, 1))  # [seq_len]\n",
        "\n",
        "        print(f\"Layer {layer_idx}:\")\n",
        "        for start in [0, 25, 50, 75]:\n",
        "            end = min(start + 25, len(kl_per_position))\n",
        "            segment_kl = kl_per_position[start:end].mean()\n",
        "            print(f\"  Positions {start:2d}-{end:2d}: KL = {segment_kl:.4f}\")\n",
        "\n",
        "    # Calculate total possible blocks for pruning percentage calculation\n",
        "    # This is (batch_size * num_heads * seq_len_q) * num_key_blocks for *each layer*\n",
        "    # The outer loop is for layers, so we sum this up for all layers.\n",
        "    # Since seq_len_q, seq_len_k, batch_size, num_heads are consistent across layers for the same input\n",
        "    # We can calculate it based on the dimensions of the first attention layer.\n",
        "\n",
        "    first_attn_shape = attentions[0].shape\n",
        "    total_possible_blocks_per_layer = first_attn_shape[0] * first_attn_shape[1] * first_attn_shape[2] * ((first_attn_shape[-1] + block_size - 1) // block_size)\n",
        "    total_blocks = total_possible_blocks_per_layer * len(attentions) # Total over all layers\n",
        "\n",
        "    return {\n",
        "        \"baseline_logits\": baseline_logits,\n",
        "        \"baseline_tokens\": baseline_tokens,\n",
        "        \"attention_divergence\": attention_divergence,\n",
        "        \"mean_kl\": mean_kl,\n",
        "        \"pruned_blocks_pct\": pruned_blocks_count / total_blocks if total_blocks > 0 else 0,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # layer and attention analysis\n",
        "    r = test_quality_with_pruning(0.002,32)\n",
        "    sys.exit(0)\n",
        "\n",
        "    thresholds = [0.01, 0.005, 0.002, 0.001, 0.0005]\n",
        "\n",
        "    # size for block_size={bs}:\n",
        "    print(f\" \"*16,end=\"\")\n",
        "    for th in thresholds:\n",
        "        print(f\"{th} \",end=\"\")\n",
        "    print()\n",
        "\n",
        "    for block_size in [16, 32]:\n",
        "        print(f\"block_size={block_size}: \",end=\"\")\n",
        "        for th in thresholds:\n",
        "            results = test_quality_with_pruning(th, block_size=bs)\n",
        "            print(f\"{results['mean_kl']:.4f} ({results['pruned_blocks_pct']:.1%}) \",end=\"\")\n",
        "        print()\n",
        "        #results = test_quality_with_pruning(0.01, block_size=bs)\n",
        "        # print(f\"block_size={bs}: kl={results['mean_kl']:.4f}\")\n",
        "\n",
        "    sys.exit(0)\n",
        "\n",
        "    # Define a list of pruning_threshold values to test\n",
        "\n",
        "    pruning_thresholds_to_test = [0.01, 0.005, 0.001, 0.0005]\n",
        "\n",
        "    for threshold in pruning_thresholds_to_test:\n",
        "        print(\"=\"*50)\n",
        "        print(f\"EVALUATING PRUNING THRESHOLD: {threshold}\")\n",
        "        print(\"=\"*50)\n",
        "        results = test_quality_with_pruning(threshold)\n",
        "        print(f\"Blocks pruned: {results['pruned_blocks_pct']:.1%}\")\n",
        "        print(f\"Attention divergence (mean KL): {results['mean_kl']:.4f}\")\n",
        "        if results['mean_kl'] < 0.1:\n",
        "            print(\"Verdict: ✅ Acceptable KL divergence (below 0.1)\")\n",
        "        else:\n",
        "            print(\"Verdict: ❌ High KL divergence (above 0.1)\")\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffab2a52"
      },
      "source": [
        "## Identify Optimal Pruning Configuration\n",
        "\n",
        "### Subtask:\n",
        "Based on the results from the different pruning thresholds, identify the highest pruning threshold that still results in a mean KL divergence less than 0.1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5f4df85"
      },
      "source": [
        "### Optimal Pruning Threshold Identification\n",
        "\n",
        "After running the `test_quality_with_pruning` function with various `pruning_threshold` values, we can analyze the results to find the optimal configuration.\n",
        "\n",
        "Here's a summary of the outcomes:\n",
        "\n",
        "*   **Pruning Threshold: 0.01**\n",
        "    *   Blocks pruned: 46.9%\n",
        "    *   Attention divergence (mean KL): 0.0100\n",
        "    *   Verdict: ✅ Acceptable KL divergence (below 0.1)\n",
        "\n",
        "*   **Pruning Threshold: 0.005**\n",
        "    *   Blocks pruned: 46.0%\n",
        "    *   Attention divergence (mean KL): 0.0034\n",
        "    *   Verdict: ✅ Acceptable KL divergence (below 0.1)\n",
        "\n",
        "*   **Pruning Threshold: 0.001**\n",
        "    *   Blocks pruned: 44.9%\n",
        "    *   Attention divergence (mean KL): 0.0001\n",
        "    *   Verdict: ✅ Acceptable KL divergence (below 0.1)\n",
        "\n",
        "*   **Pruning Threshold: 0.0005**\n",
        "    *   Blocks pruned: 44.8%\n",
        "    *   Attention divergence (mean KL): 0.0000\n",
        "    *   Verdict: ✅ Acceptable KL divergence (below 0.1)\n",
        "\n",
        "All tested thresholds resulted in a mean KL divergence well below the target of 0.1. To maximize pruning while maintaining quality, we should select the highest threshold that meets this condition.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The highest `pruning_threshold` that results in a mean KL divergence less than 0.1 is **0.01**. This threshold achieved a pruning of **46.9% of blocks** with a mean KL divergence of **0.0100**, indicating minimal impact on the attention distribution and thus minimal expected quality loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e16618b"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings, specifically highlighting the `pruning_threshold` that allows for significant sparsity while maintaining a mean KL divergence below 0.1, thus proving minimal quality loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08efd468"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The `pruning_threshold` that allows for significant sparsity while maintaining a mean KL divergence below 0.1 is 0.01. This threshold achieved 46.9% sparsity with a mean KL divergence of 0.0100, which is well below the target.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   An initial `pruning_threshold` of 0.1 resulted in 58.1% of blocks being pruned but led to an extremely high mean KL divergence of 38.3316. This significant divergence indicated substantial quality loss, primarily due to aggressive zeroing of attention blocks followed by a local renormalization process that distorted the original attention distributions.\n",
        "*   During the iterative testing, issues with the KL divergence calculation (e.g., negative values) were identified. This was resolved by switching from `torch.nn.functional.kl_div` to a manual calculation with numerical stability measures (clamping and `torch.where`) and, crucially, by implementing a global renormalization strategy for pruned attention weights, ensuring each attention row still summed to 1 after pruning.\n",
        "*   When evaluating various `pruning_threshold` values with the corrected methodology:\n",
        "    *   A `pruning_threshold` of 0.01 resulted in 46.9% of blocks being pruned, with a mean KL divergence of 0.0100.\n",
        "    *   A `pruning_threshold` of 0.005 resulted in 46.0% of blocks being pruned, with a mean KL divergence of 0.0034.\n",
        "    *   A `pruning_threshold` of 0.001 resulted in 44.9% of blocks being pruned, with a mean KL divergence of 0.0001.\n",
        "    *   A `pruning_threshold` of 0.0005 resulted in 44.8% of blocks being pruned, with a mean KL divergence of 0.0000.\n",
        "*   All tested thresholds (0.01, 0.005, 0.001, 0.0005) successfully yielded a mean KL divergence well below the target of 0.1, indicating minimal disruption to the attention distribution and thus minimal quality loss.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The `pruning_threshold` of 0.01 provides the best trade-off, achieving the highest sparsity (46.9% of blocks pruned) among the tested values while maintaining a very low mean KL divergence of 0.0100, validating its potential for efficient model compression.\n",
        "*   The next step should involve applying the identified optimal pruning strategy to the model and evaluating its end-to-end performance on relevant downstream tasks (e.g., perplexity, accuracy) to confirm that the observed minimal KL divergence truly translates to negligible quality degradation in practice.\n"
      ]
    }
  ]
}