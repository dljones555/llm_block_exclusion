{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/Avf9LaO+WLtmkIyTXFB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dljones555/llm_block_exclusion/blob/main/exclusion_routing_triton_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z6NzHoh8iRB",
        "outputId": "46e4bf8f-c4b9-4c7e-99c8-452a4014c3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing baseline with seq_len=2048\n",
            "-----\n",
            "Baseline full matmul time: 0.001669s\n",
            "Test_kernel with seq_len=2048\n",
            "-----\n",
            "Kernel time: 0.003026s\n",
            "Excluded tokens: 79.05%\n",
            "Testing baseline with seq_len=4096\n",
            "-----\n",
            "Baseline full matmul time: 0.004678s\n",
            "Test_kernel with seq_len=4096\n",
            "-----\n",
            "Kernel time: 0.000064s\n",
            "Excluded tokens: 79.83%\n",
            "Testing baseline with seq_len=37268\n",
            "-----\n",
            "Baseline full matmul time: 0.268003s\n",
            "Test_kernel with seq_len=37268\n",
            "-----\n",
            "Kernel time: 0.001621s\n",
            "Excluded tokens: 79.42%\n"
          ]
        }
      ],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "\n",
        "@triton.jit\n",
        "def exclude_kernel(\n",
        "    headers_ptr,\n",
        "    mask_ptr,\n",
        "    seq_len,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < seq_len\n",
        "\n",
        "    # Load headers (4 rows packed sequentially)\n",
        "    h_type   = tl.load(headers_ptr + offsets, mask=mask, other=0)\n",
        "    h_afford = tl.load(headers_ptr + seq_len + offsets, mask=mask, other=0)\n",
        "    h_energy = tl.load(headers_ptr + 2 * seq_len + offsets, mask=mask, other=0)\n",
        "    h_age    = tl.load(headers_ptr + 3 * seq_len + offsets, mask=mask, other=0)\n",
        "\n",
        "    # Simple exclusion logic (vectorized)\n",
        "    compat = ((h_type & h_afford) != 0) & (h_energy >= 3) & (h_age <= 5)\n",
        "\n",
        "    # Store 1 = keep, 0 = excluded\n",
        "    tl.store(mask_ptr + offsets, compat.to(tl.int32), mask=mask)\n",
        "\n",
        "def test_baseline(seq_len=64, device=\"cuda\"):\n",
        "\n",
        "    print(f\"Testing baseline with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    d_head = 64\n",
        "\n",
        "    q = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    k = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # pure matmul timing - no softmax\n",
        "    dummy_scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "    # dummy_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_head)\n",
        "\n",
        "    # dummy_attn = F.softmax(dummy_scores, dim=-1)\n",
        "    dummy_attn = torch.nn.functional.scaled_dot_product_attention(\n",
        "        q, k, k, is_causal=False\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Baseline full matmul time: {end - start:.6f}s\")\n",
        "\n",
        "# Test function (run on GPU)\n",
        "def test_kernel(seq_len=64):\n",
        "\n",
        "    print(f\"Test_kernel with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    device = \"cuda\"\n",
        "\n",
        "    headers = torch.randint(\n",
        "        0, 10, (4 * seq_len,),\n",
        "        device=device,\n",
        "        dtype=torch.int32\n",
        "    )\n",
        "\n",
        "    mask = torch.zeros(seq_len, device=device, dtype=torch.int32)\n",
        "\n",
        "    BLOCK_SIZE = 32\n",
        "    grid = (triton.cdiv(seq_len, BLOCK_SIZE),)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    exclude_kernel[grid](\n",
        "        headers,\n",
        "        mask,\n",
        "        seq_len,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "    )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Kernel time: {end - start:.6f}s\")\n",
        "    sparsity = (mask == 0).float().mean().item() * 100\n",
        "    print(f\"Excluded tokens: {sparsity:.2f}%\")\n",
        "\n",
        "seq_len=2048\n",
        "test_baseline(seq_len)\n",
        "test_kernel(seq_len)\n",
        "\n",
        "seq_len=4096\n",
        "test_baseline(seq_len)\n",
        "test_kernel(seq_len)\n",
        "\n",
        "#seq_len=8192\n",
        "#test_baseline(seq_len)\n",
        "#test_kernel(seq_len)\n",
        "\n",
        "sql_len=37268\n",
        "test_baseline(sql_len)\n",
        "test_kernel(sql_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GvLYmZjdYUQH"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}